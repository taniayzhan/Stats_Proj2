---
title: "eda_proj2"
author: "YZ"
date: "3/12/2020"
output:
  html_document:
    code_folding: "hide"
---
## Exploratory Analysis


### Observations:
#### Load in the bank-additional-full.csv dataset
#### A total of 21 columns


```{r setup, include=FALSE}
library(data.table) #for %like% function
library(xml2)
library(purrr)
library(lubridate)
library(tictoc)
library(kableExtra)
library(gridExtra)
library(readr)
library(caret)
library(MASS)
library(car)

# import report data:
bank <- read_delim("C:/Users/taniat470s/Desktop/SMU_course/DS6372/bank-additional/bank-additional-full.csv", delim = ";")
```

```{r EDA, echo=TRUE}

# Yang EDA
summary(bank)

unique(bank$job)
unique(bank$marital)
unique(bank$education)
unique(bank$default)
unique(bank$housing)
unique(bank$loan)
unique(bank$contact)
unique(bank$month)
unique(bank$poutcome)
unique(bank$y)

```

### Observations:
#### Category variables include: job, marital, education, default, housing, loan, contact, month, day_of_week, poutcome, y
#### Continues variables include: age, duration, campaign, pdays, previous, emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed
#### No NAs within the dataset

```{r Overview, echo=TRUE}
library(GGally)
library(forcats)
library(dplyr)

#data dimensions
dim(bank)

#str(reports[,c(1:10,23)])

# Print out a summary of the dataset
bank %>% glimpse()

#Check NA values
print(colSums(is.na(bank)))
```


### Observations:
#### Histograms of the continous variables display vary distributions.
#### Age with a wide close-to-normal distribution leaning lightly to younger ages.
#### Duration, campaign, previous are strongly leaning to the left, and may need a log transformation.
#### Pdays just two values, should be considered to switch to a factor-based variable.
#### The rest variables are quite discrete from their distribution.


```{r Histogram, echo=TRUE}
ggplot(data=bank[!is.na(bank$age),], aes(x=age)) +
        geom_histogram(fill="blue", binwidth = 1) + labs(title="Age Histogram", x="Age")

ggplot(data=bank[!is.na(bank$duration),], aes(x=duration)) +
        geom_histogram(fill="blue", binwidth = 10) + labs(title="Duration Histogram", x="Duration")

ggplot(data=bank[!is.na(bank$campaign),], aes(x=campaign)) +
        geom_histogram(fill="blue", binwidth = 1) + labs(title="Campaign Histogram", x="Campaign")

ggplot(data=bank[!is.na(bank$pdays),], aes(x=pdays)) +
        geom_histogram(fill="blue", binwidth = 10) + labs(title="Pdays Histogram", x="Pdays")

ggplot(data=bank[!is.na(bank$previous),], aes(x=previous)) +
        geom_histogram(fill="blue", binwidth = 1) + labs(title="Previous Histogram", x="Previous")

ggplot(data=bank[!is.na(bank$emp.var.rate),], aes(x=emp.var.rate)) +
        geom_histogram(fill="blue", binwidth = 0.1) + labs(title="Emp.var.rate Histogram", x="Emp.var.rate")

ggplot(data=bank[!is.na(bank$cons.price.idx),], aes(x=cons.price.idx)) +
        geom_histogram(fill="blue", binwidth = 0.1) + labs(title="Cons.price.idx Histogram", x="Cons.price.idx")

ggplot(data=bank[!is.na(bank$cons.conf.idx),], aes(x=cons.conf.idx)) +
        geom_histogram(fill="blue", binwidth = 0.1) + labs(title="Cons.conf.idx Histogram", x="Cons.conf.idx")

ggplot(data=bank[!is.na(bank$euribor3m),], aes(x=euribor3m)) +
        geom_histogram(fill="blue", binwidth = 0.1) + labs(title="Euribor3m Histogram", x="Euribor3m")

ggplot(data=bank[!is.na(bank$nr.employed),], aes(x=nr.employed)) +
        geom_histogram(fill="blue", binwidth = 10) + labs(title="Nr.employed Histogram", x="Nr.employed")

```


### Observations:
#### Continuous variables are plotted by contrasting y, some show obvious differences some don't.
#### Some variables the showed differences: Duration, Emp.var.rate, Cons.price,idx, Ons.conf.idx, Euribor3m, Nr.employed 
#### Some variables don't show obvious differences:Age, Campaign, Previous


```{r ContrastY, echo=TRUE}
g1<- bank %>% ggplot(aes(x=age, fill=y)) + geom_density(alpha = 0.7) + labs(title="Age Distribution colored by Term Deposit") 

g2<- bank %>% ggplot(aes(x=log(duration), fill=y)) + geom_density(alpha = 0.7) + labs(title="Duration Distribution colored by Term Deposit") 

g3<- bank %>% ggplot(aes(x=log(campaign), fill=y)) + geom_density(alpha = 0.7) + labs(title="Campaign Distribution colored by Term Deposit") 

g4<- bank %>% ggplot(aes(x=previous, fill=y)) + geom_density(alpha = 0.7) + labs(title="Previous Distribution colored by Term Deposit") 

g5<- bank %>% ggplot(aes(x=emp.var.rate, fill=y)) + geom_density(alpha = 0.7) + labs(title="Emp.var.rate Distribution colored by Term Deposit") 

g6<- bank %>% ggplot(aes(x=cons.price.idx, fill=y)) + geom_density(alpha = 0.7) + labs(title="Cons.price.idx Distribution colored by Term Deposit") 

g7<- bank %>% ggplot(aes(x=cons.conf.idx, fill=y)) + geom_density(alpha = 0.7) + labs(title="Ons.conf.idx Distribution colored by Term Deposit") 

g8<- bank %>% ggplot(aes(x=euribor3m, fill=y)) + geom_density(alpha = 0.7) + labs(title="Euribor3m Distribution colored by Term Deposit") 

g9<- bank %>% ggplot(aes(x=nr.employed, fill=y)) + geom_density(alpha = 0.7) + labs(title="Nr.employed Distribution colored by Term Deposit") 

grid.arrange(g1,g2,g3,g4,g5,g6,g7,g8,g9,ncol = 3, nrow = 3)

```

### Observations:
#### We don't observe strong linear relationship by cross-plot all continuous variables.
#### We observed some seperation in cross-plot colored by variable y.

```{r Pairs, echo=TRUE}
bank_con <- bank %>% dplyr::select(age, duration,campaign,pdays,previous,emp.var.rate,cons.price.idx,cons.conf.idx,euribor3m,nr.employed) 

#pairs(bank_con)

table(bank$y)

bank_cony <- bank %>% dplyr::select(age, duration,campaign,pdays,previous,emp.var.rate,cons.price.idx,cons.conf.idx,euribor3m,nr.employed,y) 

bank_cony$y <- factor(bank_cony$y)

#pairs(bank_cony[,-11], col=bank_cony$y)
```

### Observations:
#### Crossplots of some continuous variables to identify the seperation by variable y. Age vs Duration shows seperation in duration. Campaign vs Duration also shows seperation in duration.
#### Other crossplots doesn't show obvious seperation by variable y.

```{r Crossplot, echo=TRUE}
bank %>% dplyr::select(age, duration,y) %>%
ggplot(aes(x=age, y=log(duration)))+ geom_point(aes(colour = factor(y))) + labs(title="Age vs  Duration", x="age",y="duration") 

bank %>% dplyr::select(duration,campaign,y) %>%
ggplot(aes(x=log(duration), y=log(campaign)))+ geom_point(aes(colour = factor(y))) + labs(title="Duration vs  Campaign", x="duration",y="campaign") 

bank %>% dplyr::select(age, previous,y) %>%
ggplot(aes(x=age, y=log(previous)))+ geom_point(aes(colour = factor(y))) + labs(title="Age vs  Previous", x="age",y="previous") 

bank %>% dplyr::select(emp.var.rate, cons.price.idx,y) %>%
ggplot(aes(x=emp.var.rate, y=cons.price.idx))+ geom_point(aes(colour = factor(y))) + labs(title="emp.var.rate vs  cons.price.idx", x="emp.var.rate",y="cons.price.idx") 

bank %>% dplyr::select(cons.conf.idx, cons.price.idx,y) %>%
ggplot(aes(x=cons.conf.idx, y=cons.price.idx))+ geom_point(aes(colour = factor(y))) + labs(title="cons.conf.idx vs  cons.price.idx", x="cons.conf.idx",y="cons.price.idx") 

```

### Observations:
#### We did pca on the continous data group. Good seperation on especially PC2.
#### PC1 is a combination of emp.var.rate,cons.price.idx,euribor3m,nr.employed.
#### PC2 is a combination of pdays and previous.

```{r PCA1, echo=TRUE}
pc.bc<-prcomp(bank_con,scale.=TRUE)
pc.bc.scores<-pc.bc$x

#Adding the response column to the PC's data frame
pc.bc.scores<-data.frame(pc.bc.scores)
pc.bc.scores$y<-bank$y

#Use ggplot2 to plot the first few pc's
library(ggplot2)
ggplot(data = pc.bc.scores, aes(x = PC1, y = PC2)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of Bank Dataset PC1 and PC2")

ggplot(data = pc.bc.scores, aes(x = PC2, y = PC3)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of Bank Dataset PC2 and PC3")

pc.bc$rotation
```

#### PCA results shows 7 PCs can explain 95% of the variations

```{r Scree1, echo=TRUE}
par(mfrow=c(1,2))
eigenvals<-(pc.bc$sdev)^2
plot(1:10,eigenvals/sum(eigenvals),type="l",main="Scree Plot",ylab="Prop. Var. Explained")
cumulative.prop<-cumsum(eigenvals/sum(eigenvals))
plot(1:10,cumulative.prop,type="l",main="Cumulative proportion",ylim=c(0,1))
cumulative.prop
```


### Observations:
#### Because pdays is not a typical continous feature, we did pca on the continous data group excluding this variable. Good seperation on PC2 and PC3
#### PC2 is a combination of age and cons.conf.idx.
#### PC3 is a combination of duration and campaign.

```{r PCA2, echo=TRUE}
bank_con2 <- bank %>% dplyr::select(age, duration,campaign,previous,emp.var.rate,cons.price.idx,cons.conf.idx,euribor3m,nr.employed) 

pc.bc2<-prcomp(bank_con2,scale.=TRUE)
pc.bc2.scores<-pc.bc2$x

#Adding the response column to the PC's data frame
pc.bc2.scores<-data.frame(pc.bc2.scores)
pc.bc2.scores$y<-bank$y

#Use ggplot2 to plot the first few pc's
library(ggplot2)
ggplot(data = pc.bc2.scores, aes(x = PC1, y = PC2)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of Bank Dataset PC1 and PC2")

ggplot(data = pc.bc2.scores, aes(x = PC2, y = PC3)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of Bank Dataset PC2 and PC3")

pc.bc2$rotation
```

#### PCA results shows 6 PCs can explain 95% of the variations

```{r Scree2, echo=TRUE}
par(mfrow=c(1,2))
eigenvals<-(pc.bc2$sdev)^2
plot(1:9,eigenvals/sum(eigenvals),type="l",main="Scree Plot",ylab="Prop. Var. Explained")
cumulative.prop<-cumsum(eigenvals/sum(eigenvals))
plot(1:9,cumulative.prop,type="l",main="Cumulative proportion",ylim=c(0,1))
cumulative.prop
```


### Observations:
#### LDA with all exist continuous variables can get an accuracy of 90.7%.

````{r LDA1, echo=TRUE}
### LDA with all continuous variables
library(MASS)

mylda1 <- lda(bank$y ~ . , data = bank_con)

x1=table(predict(mylda1, type="class")$class, bank$y)
x1
#presicion
x1[1,1]/sum(x1[1,1:2])

#recall
x1[1,1]/sum(x1[1:2,1])

# F score

(2*(x1[1,1]/sum(x1[1,1:2])) * (x1[1,1]/sum(x1[1:2,1])))/(x1[1,1]/sum(x1[1,1:2])) + (x1[1,1]/sum(x1[1:2,1]))

#Missclassification Error
ME1<-(x1[2,1]+x1[1,2])/dim(bank_con)[1]
ME1
#Calculating overall accuracy
1-ME1

```


### Observations:
#### LDA with all exist continuous variables can get an accuracy of 90.6%.

````{r LDA2, echo=TRUE}
### LDA with all continuous variables excluding pdays

mylda2 <- lda(bank$y ~ . , data = bank_con2)

x1=table(predict(mylda2, type="class")$class, bank$y)
x1
#presicion
x1[1,1]/sum(x1[1,1:2])

#recall
x1[1,1]/sum(x1[1:2,1])

# F score

(2*(x1[1,1]/sum(x1[1,1:2])) * (x1[1,1]/sum(x1[1:2,1])))/(x1[1,1]/sum(x1[1,1:2])) + (x1[1,1]/sum(x1[1:2,1]))

#Missclassification Error
ME1<-(x1[2,1]+x1[1,2])/dim(bank_con2)[1]
ME1
#Calculating overall accuracy
1-ME1

```


### Observations:
#### LDA with selected continuous variables can get an accuracy of 90.7%.

````{r LDA3, echo=TRUE}
### LDA with all continuous variables selected from Balaji's work

bank_con_select = bank %>% dplyr::select(c('age', 'duration', 'campaign', 'pdays', 'previous',  'cons.price.idx', 'cons.conf.idx',
                                       'euribor3m'))


mylda3 <- lda(bank$y ~ . , data = bank_con_select)

x1=table(predict(mylda3, type="class")$class, bank$y)
x1
#presicion
x1[1,1]/sum(x1[1,1:2])

#recall
x1[1,1]/sum(x1[1:2,1])

# F score

(2*(x1[1,1]/sum(x1[1,1:2])) * (x1[1,1]/sum(x1[1:2,1])))/(x1[1,1]/sum(x1[1,1:2])) + (x1[1,1]/sum(x1[1:2,1]))

#Missclassification Error
ME1<-(x1[2,1]+x1[1,2])/dim(bank_con_select)[1]
ME1
#Calculating overall accuracy
1-ME1

```

### Observations:
#### PCA with selected continuous variables show some good seperation in PC1 vs PC2 and PC2 vs PC3.
#### PC1 is a combination of pdays, previous, cons.price.idx and eruibor3m.
#### PC2 is a combination of pdays, cons.conf.idx.
#### PC3 is a combination of age, duration and campaign.

```{r PCA3, echo=TRUE}
pc.bc3<-prcomp(bank_con_select,scale.=TRUE)
pc.bc3.scores<-pc.bc3$x

#Adding the response column to the PC's data frame
pc.bc3.scores<-data.frame(pc.bc3.scores)
pc.bc3.scores$y<-bank$y

#Use ggplot2 to plot the first few pc's
ggplot(data = pc.bc3.scores, aes(x = PC1, y = PC2)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of Bank Dataset PC1 and PC2")

ggplot(data = pc.bc3.scores, aes(x = PC2, y = PC3)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of Bank Dataset PC2 and PC3")

pc.bc3$rotation
```

#### PCA results shows 6 PCs can explain 92% of the variations
```{r Scree3, echo=TRUE}
par(mfrow=c(1,2))
eigenvals<-(pc.bc3$sdev)^2
plot(1:8,eigenvals/sum(eigenvals),type="l",main="Scree Plot",ylab="Prop. Var. Explained")
cumulative.prop<-cumsum(eigenvals/sum(eigenvals))
plot(1:8,cumulative.prop,type="l",main="Cumulative proportion",ylim=c(0,1))
cumulative.prop
```

### LDA with 6 PCs as variables give an accuracy of 90.4%.

````{r LDA4, echo=TRUE}


mylda4 <- lda(bank$y ~ PC1+PC2+PC3+PC4+PC5+PC6 , data = pc.bc3.scores)

x1=table(predict(mylda4, type="class")$class, bank$y)
x1
#presicion
x1[1,1]/sum(x1[1,1:2])

#recall
x1[1,1]/sum(x1[1:2,1])

# F score

(2*(x1[1,1]/sum(x1[1,1:2])) * (x1[1,1]/sum(x1[1:2,1])))/(x1[1,1]/sum(x1[1,1:2])) + (x1[1,1]/sum(x1[1:2,1]))

#Missclassification Error
ME1<-(x1[2,1]+x1[1,2])/dim(bank_con_select)[1]
ME1
#Calculating overall accuracy
1-ME1

```

### LDA with only the first 2 PCs as variables shows a linear cutoff line below.
```{r LDA5, echo=TRUE}
# construct the LDA model
mylda5 <- lda(bank$y ~ PC1+PC2, data = pc.bc3.scores)

# draw discrimination line
np <- 300
nd.x <- seq(from = min(pc.bc3.scores$PC1), to = max(pc.bc3.scores$PC1), length.out = np)
nd.y <- seq(from = min(pc.bc3.scores$PC2), to = max(pc.bc3.scores$PC2), length.out = np)
nd <- expand.grid(PC1 = nd.x, PC2 = nd.y)

prd <- as.numeric(predict(mylda5, newdata = nd)$class)

plot(pc.bc3.scores[, 1:2], col = as.factor(bank$y), main="LDA with PC1 and PC2")
points(mylda5$means, pch = "+", cex = 2, col = c("black", "red"))
contour(x = nd.x, y = nd.y, z = matrix(prd, nrow = np, ncol = np), 
        levels = c(1, 2), add = TRUE, drawlabels = FALSE)
```


### LDA with only PC2 and PC3 as variables shows a linear cutoff line below.
```{r LDA6, echo=TRUE}
# construct the LDA model
mylda6 <- lda(bank$y ~ PC2+PC3, data = pc.bc3.scores)

# draw discrimination line
np <- 300
nd.x <- seq(from = min(pc.bc3.scores$PC2), to = max(pc.bc3.scores$PC2), length.out = np)
nd.y <- seq(from = min(pc.bc3.scores$PC3), to = max(pc.bc3.scores$PC3), length.out = np)
nd <- expand.grid(PC2 = nd.x, PC3 = nd.y)

prd <- as.numeric(predict(mylda6, newdata = nd)$class)

plot(pc.bc3.scores[, 2:3], col = as.factor(bank$y), main="LDA with PC2 and PC3")
points(mylda6$means, pch = "+", cex = 2, col = c("black", "red"))
contour(x = nd.x, y = nd.y, z = matrix(prd, nrow = np, ncol = np), 
                levels = c(1, 2), add = TRUE, drawlabels = FALSE)
```


### QDA with only the first 2 PCs as variables shows a nonlinear cutoff line below.
```{r QDA1, echo=TRUE}
# construct the QDA model
myqda <- qda(bank$y ~ PC1+PC2, data = pc.bc3.scores)

# draw discrimination line
np <- 300
nd.x <- seq(from = min(pc.bc3.scores$PC1), to = max(pc.bc3.scores$PC1), length.out = np)
nd.y <- seq(from = min(pc.bc3.scores$PC2), to = max(pc.bc3.scores$PC2), length.out = np)
nd <- expand.grid(PC1 = nd.x, PC2 = nd.y)

prd <- as.numeric(predict(myqda, newdata = nd)$class)

plot(pc.bc3.scores[, 1:2], col = as.factor(bank$y), main="QDA with PC1 and PC2")
points(myqda$means, pch = "+", cex = 2, col = c("black", "red"))
contour(x = nd.x, y = nd.y, z = matrix(prd, nrow = np, ncol = np), 
                levels = c(1, 2), add = TRUE, drawlabels = FALSE)
```

### QDA with only PC2 and PC3 as variables shows a non-linear cutoff line below.
```{r QDA2, echo=TRUE}
# construct the QDA model
myqda2 <- qda(bank$y ~ PC2+PC3, data = pc.bc3.scores)

# draw discrimination line
np <- 300
nd.x <- seq(from = min(pc.bc3.scores$PC2), to = max(pc.bc3.scores$PC2), length.out = np)
nd.y <- seq(from = min(pc.bc3.scores$PC3), to = max(pc.bc3.scores$PC3), length.out = np)
nd <- expand.grid(PC2 = nd.x, PC3 = nd.y)

prd <- as.numeric(predict(myqda2, newdata = nd)$class)

plot(pc.bc3.scores[, 2:3], col = as.factor(bank$y), main="QDA with PC2 and PC3")
points(myqda2$means, pch = "+", cex = 2, col = c("black", "red"))
contour(x = nd.x, y = nd.y, z = matrix(prd, nrow = np, ncol = np), 
                levels = c(1, 2), add = TRUE, drawlabels = FALSE)
```
        
### QDA with all continuous variables give an accuracy of 88.9%.

````{r QDA3, echo=TRUE}


myqda3 <- qda(bank$y ~ PC1+PC2+PC3+PC4+PC5+PC6 , data = pc.bc3.scores)

x1=table(predict(myqda3, type="class")$class, bank$y)
x1
#presicion
x1[1,1]/sum(x1[1,1:2])

#recall
x1[1,1]/sum(x1[1:2,1])

# F score

(2*(x1[1,1]/sum(x1[1,1:2])) * (x1[1,1]/sum(x1[1:2,1])))/(x1[1,1]/sum(x1[1,1:2])) + (x1[1,1]/sum(x1[1:2,1]))

#Missclassification Error
ME1<-(x1[2,1]+x1[1,2])/dim(bank_con_select)[1]
ME1
#Calculating overall accuracy
1-ME1

```

### QDA with all continuous variables give an accuracy of 87.7%.

```{r QDA4, echo=TRUE}

myqda4 <- qda(bank$y ~ . , data = bank_con2)

x1=table(predict(myqda4, type="class")$class, bank$y)

x1_con <- confusionMatrix(x1)
x1_con

x1_con$overall[1]
x1_con$byClass[1]
x1_con$byClass[2]


```


#### Creating a balanced dataset
### First of all let's create a new dataset with our class balanced with oversampling and also with undersampling 
## Splitting the data into training and test datasets (80-20 split)

```{r}
bank$y <- as.factor(bank$y)
bankdata <- bank

# dataset with "no"
bankdata_no = bankdata[which(bankdata$y=="no"),]
# dataset with "yes"
bankdata_yes = bankdata[which(bankdata$y=="yes"),]

splitPerc = .7 #Training / Test split Percentage

# Training / Test split of "no" dataset
noIndices = sample(1:dim(bankdata_no)[1],round(splitPerc * dim(bankdata_no)[1]))
train_no = bankdata_no[noIndices,]
test_no = bankdata_no[-noIndices,]

# Training / Test split of "yes" dataset
yesIndices = sample(1:dim(bankdata_yes)[1],round(splitPerc * dim(bankdata_yes)[1]))
train_yes = bankdata_yes[yesIndices,]
test_yes = bankdata_yes[-yesIndices,]

# Combine training "no" and "yes"
bank_train = rbind(train_no, train_yes)

# Combine test "no" and "yes"
bank_test = rbind(test_no, test_yes)

# remove bservations with "unknown" in the "Loan" variable
bank_train = bank_train[-(which(bank_train$loan=="unknown")), ]
bank_train = bank_train[-(which(bank_train$default=="yes")), ]

# upsampling training dataset
bank_train_upsample <- upSample(x = bank_train[, -ncol(bank_train)], y = as.factor(bank_train$y))
colnames(bank_train_upsample)[21] <- "y"
summary(bank_train_upsample$y)

# remove bservations with "unknown" in the "Loan" variable
#bank_train_upsample = bank_train_upsample[-(which(bank_train_upsample$loan=="unknown")), ]

# remove bservations with "unknown" in the "Loan" variable
bank_test = bank_test[-(which(bank_test$loan=="unknown")), ]
bank_test = bank_test[-(which(bank_test$default=="yes")), ]

model.main<-glm(y ~ . , data=bank_train_upsample,family = binomial(link="logit"))
(vif(model.main)[,3])^2

## Below variables has VIF more than 10 
## nr.employed - 145.027646
## euribor3m    - 144.159591
## emp.var.rate - 132.439592
## cons.price.idx - 55.437501

model.main1<-glm(y ~ age+job+marital+education+default+housing+loan+contact+month+day_of_week+duration+campaign+pdays+previous+poutcome+ cons.conf.idx, data=bank_train_upsample,family = binomial(link="logit"))

(vif(model.main1)[,3])^2
summary(model.main1)

exp(cbind("Odds ratio" = coef(model.main1), confint.default(model.main1, level = 0.95)))

```

```{r}
#--- CROSS VALIDATION SETUP ---

#creates a train test split with upsampling train data for balance
split_train_test <- function(bankdata) {
  # dataset with "no"
  bankdata_no = bankdata[which(bankdata$y=="no"),]
  # dataset with "yes"
  bankdata_yes = bankdata[which(bankdata$y=="yes"),]
  
  splitPerc = .7 #Training / Test split Percentage
  
  # Training / Test split of "no" dataset
  noIndices = sample(1:dim(bankdata_no)[1],round(splitPerc * dim(bankdata_no)[1]))
  train_no = bankdata_no[noIndices,]
  test_no = bankdata_no[-noIndices,]
  
  # Training / Test split of "yes" dataset
  yesIndices = sample(1:dim(bankdata_yes)[1],round(splitPerc * dim(bankdata_yes)[1]))
  train_yes = bankdata_yes[yesIndices,]
  test_yes = bankdata_yes[-yesIndices,]
  
  # Combine training "no" and "yes"
  bank_train = rbind(train_no, train_yes)
  
  # Combine test "no" and "yes"
  bank_test = rbind(test_no, test_yes)
  
  # upsampling training dataset
  bank_train_upsample <- upSample(x = bank_train[, -ncol(bank_train)], y = as.factor(bank_train$y))
  colnames(bank_train_upsample)[21] <- "y"
  summary(bank_train_upsample$y)
  
  return(list(bank_train_upsample, bank_test))
}

#gives the average accuracy, sensitivity, and specificity over 5 train-test splits
#for logistic regression model
get_test_metrics <- function(data, formula, thresh){
  accuracies <- c()
  sensitivities <- c()
  specificities <- c()
  for(i in 1:5){
    split_data <- split_train_test(data)
    train <- split_data[[1]]
    test <- split_data[[2]]
    
    model <- glm(formula, data=train,family = binomial(link="logit"))
    pred_probs <- predict(model, test, type="response")
    pred_yns <- factor(ifelse(pred_probs>thresh, "yes", "no"))
    
    cm <- confusionMatrix(table(pred_yns, test$y))
    accuracies <- c(accuracies, cm$overall[1])
    sensitivities <- c(sensitivities, cm$byClass[1])
    specificities <- c(specificities, cm$byClass[2])
  }
  acc = mean(accuracies)
  sens = mean(sensitivities)
  specs = mean(specificities)
  result = list(acc, sens, specs)
  names(result) = c("Accuracy", "Sensitivity", "Specificity")
  return(result)
}


#see accuracy metrics for this model, at threshold of .5 for mapping yes/no
m_everything <- get_test_metrics(bank, y ~ ., .5)
m_everything

#trying with manual variable selection based on what visually looked relevant
m_manual <- get_test_metrics(bank, y ~ job + default + contact + month + poutcome + duration + emp.var.rate + cons.price.idx + euribor3m + nr.employed, .5)
m_manual
```

### Observations:
#### LDA with all variables for upsampled dataset can get an accuracy of 85.6%.

```{r LDA_new, echo=TRUE}
### LDA with all variables


mylda_new <- lda(bank_train_upsample$y ~ . , data = bank_train_upsample)

x1=table(predict(mylda_new, type="class")$class, bank_train_upsample$y)

x1_con <- confusionMatrix(x1)
x1_con

x1_con$overall[1]
x1_con$byClass[1]
x1_con$byClass[2]

# Test group confusion metric statistics
x2=table(predict(mylda_new, bank_test, type="class")$class, bank_test$y)

x2_con <- confusionMatrix(x2)
x2_con

x2_con$overall[1]
x2_con$byClass[1]
x2_con$byClass[2]

```

### Observations:
#### LDA with selected variables for upsampled dataset can get an accuracy of 85.4%.

```{r LDA_new2, echo=TRUE}
### LDA with selected variables

mylda_new2 <- lda(bank_train_upsample$y ~ job + default + contact + month + poutcome + duration + emp.var.rate + cons.price.idx + euribor3m + nr.employed, data = bank_train_upsample)

x1=table(predict(mylda_new2, type="class")$class, bank_train_upsample$y)

x1_con <- confusionMatrix(x1)
x1_con

x1_con$overall[1]
x1_con$byClass[1]
x1_con$byClass[2]

# Test group confusion metric statistics
x2=table(predict(mylda_new2, bank_test, type="class")$class, bank_test$y)

x2_con <- confusionMatrix(x2)
x2_con

x2_con$overall[1]
x2_con$byClass[1]
x2_con$byClass[2]

```

### QDA with all continuous variables from upsampled dataset give an accuracy of 82.2%.

```{r QDA_new, echo=TRUE}

bank_upsample_con <- bank_train_upsample %>% dplyr::select(age, duration,campaign,previous,emp.var.rate,cons.price.idx,cons.conf.idx,euribor3m,nr.employed,y) 

myqda_new <- qda(bank_upsample_con$y ~ . , data = bank_upsample_con)

x1=table(predict(myqda_new, type="class")$class, bank_upsample_con$y)

x1_con <- confusionMatrix(x1)
x1_con

x1_con$overall[1]
x1_con$byClass[1]
x1_con$byClass[2]

# Test group confusion metric statistics
x2=table(predict(myqda_new, bank_test, type="class")$class, bank_test$y)

x2_con <- confusionMatrix(x2)
x2_con

x2_con$overall[1]
x2_con$byClass[1]
x2_con$byClass[2]

```

```{r}
bank_train_upsample_con <- bank_train_upsample %>% dplyr::select(age, duration,campaign,previous,emp.var.rate,cons.price.idx,cons.conf.idx,euribor3m,nr.employed)

bank_test_con <- bank_test %>% dplyr::select(age, duration,campaign,previous,emp.var.rate,cons.price.idx,cons.conf.idx,euribor3m,nr.employed)
```
#### KNN model 

```{r}
library(class)

model_knn = knn(bank_train_upsample_con,bank_test_con,bank_train_upsample$y, prob = FALSE, k = 3)
table(model_knn,bank_test$y)
CM_knn = confusionMatrix(table(model_knn,bank_test$y))
CM_knn
```

```{r}
library(caret)
iterations = 10
numks = 20

masterAcc = matrix(nrow = iterations, ncol = numks)
masterSen = matrix(nrow = iterations, ncol = numks)
masterSpe = matrix(nrow = iterations, ncol = numks)
  
for(j in 1:iterations)
{
accs = data.frame(accuracy = numeric(10), k = numeric(10))
for(i in 1:numks)
{
  classifications = knn(bank_train_upsample_con,bank_test_con,bank_train_upsample$y, prob = TRUE, k = i)
  table(classifications,bank_test$y)
  CM = confusionMatrix(table(classifications,bank_test$y))
  masterAcc[j,i] = CM$overall[1]
  masterSen[j,i] = CM$byClass[1]
  masterSpe[j,i] = CM$byClass[2]
}

}

MeanAcc = colMeans(masterAcc)

plot(seq(1,numks,1),MeanAcc, type = "l")

MeanSen = colMeans(masterSen)

plot(seq(1,numks,1),MeanSen, type = "l")

MeanSpe = colMeans(masterSpe)

plot(seq(1,numks,1),MeanSpe, type = "l")
```


```{r}
library(randomForest)
bank_train_con <- bank_train %>% dplyr::select(age, duration,campaign,previous,emp.var.rate,cons.price.idx,cons.conf.idx,euribor3m,nr.employed)

## Classification Method: RandomForest
model_rf = randomForest(bank_train_con,as.factor(bank_train$y),ntree=500)
CM_rf = confusionMatrix(table(predict(model_rf,bank_test_con),bank_test$y))
CM_rf

## But random forest can provide with feature importance
varImpPlot(model_rf)
```

```{r}

## Classification Method: RandomForest
model_rf2 = randomForest(bank_train_upsample_con,as.factor(bank_train_upsample$y),ntree=500)
CM_rf2 = confusionMatrix(table(predict(model_rf2,bank_test_con),bank_test$y))
CM_rf2

## But random forest can provide with feature importance
varImpPlot(model_rf2)
```

```{r}
bank_train_upsample_rf <-  bank_train_upsample

# Make dependent variable as a factor (categorical)
bank_train_upsample_rf$job = as.factor(bank_train_upsample_rf$job)
bank_train_upsample_rf$marital = as.factor(bank_train_upsample_rf$marital)
bank_train_upsample_rf$education = as.factor(bank_train_upsample_rf$education)
bank_train_upsample_rf$default = as.factor(bank_train_upsample_rf$default)
bank_train_upsample_rf$housing = as.factor(bank_train_upsample_rf$housing)
bank_train_upsample_rf$loan = as.factor(bank_train_upsample_rf$loan)
bank_train_upsample_rf$contact = as.factor(bank_train_upsample_rf$contact)
bank_train_upsample_rf$month = as.factor(bank_train_upsample_rf$month)
bank_train_upsample_rf$day_of_week = as.factor(bank_train_upsample_rf$day_of_week)
bank_train_upsample_rf$poutcome = as.factor(bank_train_upsample_rf$poutcome)

bank_test_rf <-  bank_test

# Make dependent variable as a factor (categorical)
bank_test_rf$job = as.factor(bank_test$job)
bank_test_rf$marital = as.factor(bank_test$marital)
bank_test_rf$education = as.factor(bank_test$education)
bank_test_rf$default = as.factor(bank_test$default)
bank_test_rf$housing = as.factor(bank_test$housing)
bank_test_rf$loan = as.factor(bank_test$loan)
bank_test_rf$contact = as.factor(bank_test$contact)
bank_test_rf$month = as.factor(bank_test$month)
bank_test_rf$day_of_week = as.factor(bank_test$day_of_week)
bank_test_rf$poutcome = as.factor(bank_test$poutcome)


## Classification Method: RandomForest
model_rf_all = randomForest(bank_train_upsample_rf[,1:20],as.factor(bank_train_upsample$y),ntree=500)
CM_rf_all = confusionMatrix(table(predict(model_rf_all,bank_test_rf[,1:20]),bank_test$y))
CM_rf_all

## But random forest can provide with feature importance
varImpPlot(model_rf_all)
```

```{r}
bank_upsample_select <- bank_train_upsample_rf %>% dplyr::select(c('age', 'duration', 'campaign', 'pdays', 'previous',  'cons.price.idx', 'cons.conf.idx','euribor3m','y'))
bank_test_select <- bank_test_rf %>% dplyr::select(c('age', 'duration', 'campaign', 'pdays', 'previous',  'cons.price.idx', 'cons.conf.idx','euribor3m','y'))

## Classification Method: RandomForest
model_rf_sel = randomForest(bank_upsample_select[,1:8],as.factor(bank_upsample_select$y),ntree=500)
CM_rf_sel = confusionMatrix(table(predict(model_rf_sel,bank_test_select[,1:8]),bank_test_select$y))
CM_rf_sel

## But random forest can provide with feature importance
varImpPlot(model_rf_sel)


```
